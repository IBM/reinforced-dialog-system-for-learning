--- generation_utils_doha.py	2021-07-30 15:52:41.283000000 -0400
+++ generation_utils_new.py	2021-08-13 19:24:26.189788849 -0400
@@ -410,16 +410,31 @@
         if num_return_sequences > 1 or num_beams > 1:
             input_ids_len = input_ids.shape[-1]
             input_ids = input_ids.unsqueeze(1).expand(batch_size, effective_batch_mult * num_beams, input_ids_len)
-            attention_mask = attention_mask.unsqueeze(1).expand(
-                batch_size, effective_batch_mult * num_beams, input_ids_len
-            )
 
             input_ids = input_ids.contiguous().view(
                 effective_batch_size * num_beams, input_ids_len
             )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)
-            attention_mask = attention_mask.contiguous().view(
-                effective_batch_size * num_beams, input_ids_len
-            )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)
+
+            if type(attention_mask) is tuple:
+                assert len(attention_mask) == 2
+                a1, a2 = attention_mask
+                a1 = a1.unsqueeze(1).expand(batch_size, effective_batch_mult * num_beams, a1.shape[1])
+                a2 = a2.unsqueeze(1).expand(batch_size, effective_batch_mult * num_beams, a2.shape[1])
+                a1 = a1.contiguous().view(
+                    effective_batch_size * num_beams, a1.shape[2]
+                )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)
+                a2 = a2.contiguous().view(
+                    effective_batch_size * num_beams, a2.shape[2]
+                )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)
+                attention_mask = (a1, a2)
+            else:
+                assert torch.is_tensor(attention_mask)
+                attention_mask = attention_mask.unsqueeze(1).expand(
+                    batch_size, effective_batch_mult * num_beams, input_ids_len
+                )
+                attention_mask = attention_mask.contiguous().view(
+                    effective_batch_size * num_beams, input_ids_len
+                )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)
 
         if self.config.is_encoder_decoder:
             # create empty decoder input_ids
@@ -453,14 +468,14 @@
 
             # expand encoder_outputs
             if 'encoder_outputs' in model_kwargs:
-                
+
                 if type(encoder_outputs[1]) == type(None):
                     # takes care of multi_input case
                     encoder_outputs = (encoder_outputs[0].index_select(0, expanded_batch_idxs), None)
                 else:
                     # takes care of adapt_mha case
                     encoder_outputs = (
-                    encoder_outputs[0].index_select(0, expanded_batch_idxs), 
+                    encoder_outputs[0].index_select(0, expanded_batch_idxs),
                     encoder_outputs[1].index_select(0, expanded_batch_idxs)
                 )
             else:
